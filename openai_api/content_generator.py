import os
import logging
import json
from openai import OpenAI
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ãƒ­ã‚°è¨­å®š
os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/scraper.log", level=logging.INFO)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def get_page_source_with_age_verification(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        driver.get(url)
        driver.implicitly_wait(5)

        # å¹´é½¢ç¢ºèªã€Œã¯ã„ã€ãƒœã‚¿ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¯ãƒªãƒƒã‚¯
        try:
            yes_button = driver.find_element(By.LINK_TEXT, "ã¯ã„")
            yes_button.click()
            driver.implicitly_wait(5)
        except Exception:
            pass

        return driver.page_source
    finally:
        driver.quit()

def get_dmm_comment_text(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0",
    }
    res = requests.get(url, headers=headers, timeout=10)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, "html.parser")
    comment_div = soup.select_one("div.mg-b20.lh4")

    if comment_div:
        text = comment_div.get_text(separator="\n").strip()
        return text
    else:
        return ""

def scrape_product_details(url: str) -> dict:
    try:
        html = get_page_source_with_age_verification(url)
        soup = BeautifulSoup(html, "html.parser")

        summary_el = soup.select_one(".summary__txt")
        comment_el = soup.select_one(".trailer__txt")
        fallback_el = soup.select_one("div.mg-b20.lh4")

        summary = summary_el.get_text(strip=True) if summary_el else ""
        comment = comment_el.get_text(strip=True) if comment_el else ""

        if not summary and fallback_el:
            summary = fallback_el.get_text(separator="\n").strip()
            
            if not summary:
                # ã‚³ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯ã€åˆ¥ã®å ´æ‰€ã‹ã‚‰å–å¾—ã™ã‚‹
                # â–¼ å•†å“æƒ…å ±ã‚¨ãƒªã‚¢ã‹ã‚‰ã‚ã‚‰ã™ã˜ã‚’æŠ½å‡ºï¼ˆDMMå‹•ç”»ç”¨ï¼‰
                # ç‰¹å®šã®divæ§‹é€ ï¼š <div class="mg-b20 lh4"><p>ä½œå“èª¬æ˜</p></div>
                comment_div = get_dmm_comment_text(url)
                if comment_div:
                    summary = comment_div


        return summary

    except Exception as e:
        logging.warning(f"[Scrape Error] URL: {url} â†’ {e}")
        return ""


# --- generate_contenté–¢æ•° ---
def generate_content(item: dict) -> dict:
    title = item.get("title", "")
    genres_raw = item.get("iteminfo", {}).get("genre", [])
    genres = [g.get("name") for g in genres_raw if "name" in g]
    review_score = item.get("review", {}).get("average", "ä¸æ˜")
    review_count = item.get("review", {}).get("count", 0)
    maker_list = item.get("maker") or item.get("manufacture") or [{}]
    maker = maker_list[0].get("name", "")
    series = item.get("iteminfo", {}).get("series", [{}])[0].get("name", "")
    actresses = item.get("iteminfo", {}).get("actress", [])
    directors = item.get("iteminfo", {}).get("director", [])
    release_date = item.get("date", "")
    category_name = item.get("category_name", "")
    # HTMLã‹ã‚‰ã‚ã‚‰ã™ã˜ã‚’å–å¾—
    url = item.get("URL", "")
    html_summary = scrape_product_details(url)
    
    actress_names = [a.get("name") for a in actresses if a.get("name")]
    director_names = [d.get("name") for d in directors if d.get("name")]

    # å¥³å„ªãƒ»ç›£ç£ã®ç´¹ä»‹æ–‡ã‚’å‹•çš„ã«æ§‹ç¯‰
    cast_info = ""
    if actress_names:
        cast_info += f"- å‡ºæ¼”: {', '.join(actress_names)}\n"
    if director_names:
        cast_info += f"- ç›£ç£: {', '.join(director_names)}\n"

    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬èªã®ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€å•†å“ï¼ˆæˆäººå‘ã‘ã‚’å«ã‚€ï¼‰ã®ç´¹ä»‹æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ–‡ç« ã«ã¯æˆäººå‘ã‘ãƒ»æ€§çš„è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€å†…å®¹ã‚’æãªã‚ãšã€å…¬åºè‰¯ä¿—ã«åã—ãªã„ãƒ¬ãƒ™ãƒ«ã¾ã§ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
å„é …ç›®ã®æ–‡ã¯æ®µè½æ§‹æˆã«ã—ã¦ã€æ”¹è¡Œã—ãŸã„ç®‡æ‰€ã«ã¯ã€Œ\\n\\nã€ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯ **JSONå½¢å¼** ã§ã€æ¬¡ã®3ã¤ã®é …ç›®ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

        ---
        ### ğŸ¯ å‡ºåŠ›é …ç›®
        1. auto_commentï¼ˆ10ï½20æ–‡å­—ã®ä¸€è¨€æ„Ÿæƒ³ï¼‰
        2. auto_summaryï¼ˆã‚¸ãƒ£ãƒ³ãƒ«ã«åˆã‚ã›ãŸ100æ–‡å­—å‰å¾Œã®æ¦‚è¦ï¼‰
        3. auto_pointï¼ˆ200æ–‡å­—å‰å¾Œã®è²·ã„ãŸããªã‚‹ãƒã‚¤ãƒ³ãƒˆã€‚ç®‡æ¡æ›¸ãã§ï¼‰

        ---
        ### ğŸ§© ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã®æ–‡ä½“æŒ‡é‡
        - **AVï¼å‹•ç”»**: ã‚»ã‚¯ã‚·ãƒ¼ã•ãƒ»è‡¨å ´æ„Ÿãƒ»æ¼”å‡ºã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§è¡¨ç¾ã€‚éåº¦ã«ç›´æ¥çš„ãªæå†™ã¯ç¦æ­¢ã€‚
        - **åŒäººä½œå“**: ä½œè€…ã®å€‹æ€§ã‚„ãƒ†ãƒ¼ãƒæ€§ã‚’é‡è¦–ã€‚ä¸–ç•Œè¦³ã‚„é­…åŠ›ã‚’æƒ…æ„Ÿè±Šã‹ã«ã€‚
        - **æ¼«ç”»ãƒ»ã‚¢ãƒ‹ãƒ¡**: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã‚„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®é–¢ä¿‚æ€§ã‚’ä¸­å¿ƒã«ã€‚
        - **å†™çœŸé›†ãƒ»ã‚°ãƒ©ãƒ“ã‚¢**: ãƒ¢ãƒ‡ãƒ«ã®é­…åŠ›ã‚„é›°å›²æ°—ã€æ’®å½±ãƒ†ãƒ¼ãƒã‚’ä¸å¯§ã«è¡¨ç¾ã€‚
        - **ã‚²ãƒ¼ãƒ ç³»**: ã‚²ãƒ¼ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ»ã‚·ãƒŠãƒªã‚ªã‚’ã‚ã‹ã‚Šã‚„ã™ãè¦ç´„ã€‚

        æ–‡ä½“ã¯ã‚¸ãƒ£ãƒ³ãƒ«ã«å¿œã˜ã¦è‡ªç„¶ã«å¤‰åŒ–ã•ã›ã¦ãã ã•ã„ã€‚

        ---
        ### âš ï¸ ç¦æ­¢ãƒ«ãƒ¼ãƒ«
        - ä»¥ä¸‹ã®èªå¥ã¯ä½¿ç”¨ç¦æ­¢ï¼šã€Œä¸€å†Šã€ã€Œä½œå“ã€ã€Œä¸€ä½œã€ã€Œè©±ã€ã€Œï¼ã€  
        - ã€Œæœ¬ä½œã€ã€Œã“ã®ä½œå“ã€ãªã©ã®ãƒ†ãƒ³ãƒ—ãƒ¬çš„ãªå°å…¥ã¯ç¦æ­¢ã€‚  
        - å®Ÿéš›ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚„ã‚ã‚‰ã™ã˜ã‚’å‚è€ƒã«ã€è‡ªç„¶ãªæ–‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚  
        - å‡ºåŠ›ã¯å¿…ãš **JSONã®ã¿**ï¼ˆèª¬æ˜æ–‡ã‚„æ³¨é‡ˆã‚’å«ã‚ãªã„ï¼‰ã€‚

        ---
        ### ğŸ“¥ å…¥åŠ›æƒ…å ±
        - ã‚«ãƒ†ã‚´ãƒª: {category_name}
        - ã‚¿ã‚¤ãƒˆãƒ«: {title}
        - ã‚¸ãƒ£ãƒ³ãƒ«: {genres}
        - ãƒ¬ãƒ“ãƒ¥ãƒ¼: {review_score}ç‚¹ï¼ˆ{review_count}ä»¶ï¼‰
        - ãƒ¡ãƒ¼ã‚«ãƒ¼: {maker}
        - ç™ºå£²æ—¥: {release_date}
        - ã‚·ãƒªãƒ¼ã‚º: {series or 'è©²å½“ãªã—'}
        - å‡ºæ¼”å¥³å„ª: {cast_info}

        â–¼ HTMLã‹ã‚‰å–å¾—ã—ãŸå†…å®¹:
        {html_summary}

        ---
        ### ğŸ“¤ å‡ºåŠ›å½¢å¼ï¼ˆä¾‹ï¼‰
        ```json
        {{
        "auto_comment": "å¿ƒã‚’å¥ªã†ã»ã©æ¿ƒå¯†ãªã²ã¨ã¨ãã€‚",
        "auto_summary": "ã“ã“ã«ã‚¸ãƒ£ãƒ³ãƒ«ã«å¿œã˜ãŸç´„1000æ–‡å­—ã®æ¦‚è¦ã‚’ç”Ÿæˆã€‚",
        "auto_point": "è³¼è²·æ„æ¬²ã‚’é«˜ã‚ã‚‹ç´„500æ–‡å­—ã®ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆã€‚"
        }}
        ä¸Šè¨˜ã®å½¢å¼ã«å¾“ã„ã€JSONã¨ã—ã¦ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.85
        )

        content = response.choices[0].message.content.strip()
        if "```" in content:
            json_str = content.split("```")[1]
            if json_str.startswith("json"):
                json_str = json_str[4:].strip()
        else:
            json_str = content

        data = json.loads(json_str)

        # å„é …ç›®ã§ \\n â†’ å®Ÿéš›ã®æ”¹è¡Œ ã«å¤‰æ›
        for key in ["auto_comment", "auto_summary", "auto_point"]:
            if key in data and isinstance(data[key], str):
                data[key] = data[key].replace("\\n", "\n")

        return data

    except Exception as e:
        logging.error("[OpenAI ERROR] %s", str(e))
        return {"auto_comment": "", "auto_summary": "", "auto_point": ""}